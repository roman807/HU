{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CISC 601 - Scientific Computing II\n",
    "* Assignment: Artificial Neural Network (ANN) for activity recognition\n",
    "* Data set and problem description: https://www.neuraldesigner.com/learning/examples/activity-recognition#DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# import os\n",
    "# os.chdir('/home/roman/Documents/HU/CISC601_ScientificComputingII/Assignments/ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10299 entries, 0 to 10298\n",
      "Columns: 562 entries, tBodyAcc_mean_X to label\n",
      "dtypes: float64(561), object(1)\n",
      "memory usage: 44.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load data:\n",
    "data = pd.read_csv('activity_recognition.csv', delimiter=';')\n",
    "print(data.info())\n",
    "X = data.iloc[:, :-1]\n",
    "Y = pd.get_dummies(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran GBM done with target variable:  LAYING\n",
      "ran GBM done with target variable:  SITTING\n",
      "ran GBM done with target variable:  STANDING\n",
      "ran GBM done with target variable:  WALKING\n",
      "ran GBM done with target variable:  WALKING_DOWNSTAIRS\n"
     ]
    }
   ],
   "source": [
    "# select most important features:\n",
    "n_features = 50\n",
    "feat_importances = []\n",
    "for i in Y.columns:\n",
    "    clf = GradientBoostingClassifier()\n",
    "    clf.fit(X, Y.loc[:, i])\n",
    "    print('ran GBM done with target variable: ', i)\n",
    "    feat_importances.append(pd.Series(clf.feature_importances_, index=X.columns))\n",
    "features = pd.concat(feat_importances, axis=1).sum(axis=1).sort_values(ascending=False)[:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data set:\n",
    "X = X[features.index]\n",
    "df = pd.concat([X, Y], axis=1)\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ANN:\n",
    "def ann(indput_dim, output_dim, dropout=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(units=60, kernel_initializer='uniform', input_dim=indput_dim, activation='tanh'),\n",
    "        Dropout(dropout),\n",
    "        Dense(units=60, kernel_initializer='uniform', activation='tanh'),\n",
    "        Dropout(dropout),\n",
    "        Dense(units=25, kernel_initializer='uniform', activation='tanh'),\n",
    "        Dropout(dropout),\n",
    "#         Dense(15, kernel_initializer='uniform', activation='tanh'),\n",
    "        Dense(output_dim, kernel_initializer='uniform', activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit model:\n",
    "filepath = 'weights.best.hdf5'\n",
    "checkpoints = [ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')]\n",
    "clf = ann(\n",
    "    indput_dim=X_train.shape[1], \n",
    "    output_dim=Y_train.shape[1], \n",
    "    dropout=0.2\n",
    ")\n",
    "clf.compile(\n",
    "    optimizer=Adam(lr = 1e-3), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = clf.fit(\n",
    "    x=X_train, \n",
    "    y=Y_train, \n",
    "    validation_data=(X_val,Y_val),\n",
    "    callbacks=checkpoints,\n",
    "    batch_size=10, \n",
    "    epochs=300, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history of training and validation loss:\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.rcParams.update({'figure.figsize': [8, 5.5], 'font.size': 16})\n",
    "plt.plot(train_loss, label='training loss')\n",
    "plt.plot(val_loss, label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training vs validation loss after each epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "clf.load_weights('weights.best.hdf5')\n",
    "Y_pred_train = clf.predict_classes(X_train, verbose=1)\n",
    "Y_pred_val = clf.predict_classes(X_val, verbose=1)\n",
    "Y_pred_test = clf.predict_classes(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform Y_true and Y_pred to classes:\n",
    "classes = {}\n",
    "for i in range(len(Y.columns)):\n",
    "    classes[i] = Y.columns[i]\n",
    "Y_pred_train_classes = [classes[i] for i in Y_pred_train]\n",
    "Y_pred_val_classes = [classes[i] for i in Y_pred_val]\n",
    "Y_pred_test_classes = [classes[i] for i in Y_pred_test]\n",
    "Y_train_classes = Y_train[Y_train==1].stack().reset_index().drop(0,1)['level_1']\n",
    "Y_val_classes = Y_val[Y_val==1].stack().reset_index().drop(0,1)['level_1']\n",
    "Y_test_classes = Y_test[Y_test==1].stack().reset_index().drop(0,1)['level_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracies:\n",
    "train_accuracy = accuracy_score(Y_train_classes, Y_pred_train_classes)\n",
    "val_accuracy = accuracy_score(Y_val_classes, Y_pred_val_classes)\n",
    "test_accuracy = accuracy_score(Y_test_classes, Y_pred_test_classes)\n",
    "print('training accuracy: ', train_accuracy)\n",
    "print('validation accuracy: ', val_accuracy)\n",
    "print('test accuracy: ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
